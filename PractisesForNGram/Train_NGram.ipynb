{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "from random import random\n",
    "import pprint\n",
    "import operator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Syllable:\n",
    "    def __init__(self , text):\n",
    "        self.__words = text.split()\n",
    "    \n",
    "    #Reference for this method : https://gist.github.com/miratcan/9196ae2591b1f34ab645520a767ced17\n",
    "    def __get_syllables_word(self , word):\n",
    "        syllables = []\n",
    "\n",
    "        \"\"\"\n",
    "        Aşağıdaki satır gelen kelimenin ünlü harfler 1, ünsüzler 0 olacak\n",
    "        şekilde desenini çıkarır.\n",
    "        Örneğin: arabacı -> 1010101, türkiye -> 010010\n",
    "        \"\"\"\n",
    "\n",
    "        bits = ''.join(['1' if l in 'aeıioöuü' else '0' for l in word])\n",
    "\n",
    "        \"\"\"\n",
    "        Aşağıdaki seperators listesi, yakalanacak desenleri ve desen yakalandığında\n",
    "        kelimenin hangi pozisyondan kesileceğini tanımlıyor.\n",
    "        Türkçede kelime içinde iki ünlü arasındaki ünsüz, kendinden sonraki\n",
    "        ünlüyle hece kurar., yani 101 desenini yakaladığımızda kelimeyi\n",
    "        bulunduğumuz yerden 1 ileri pozisyondan kesmeliyiz. ('101', 1)\n",
    "        Kelime içinde yan yana gelen iki ünsüzden ilki kendinden önceki ünlüyle,\n",
    "        ikincisi kendinden sonraki ünlüyle hece kurar. Bu da demek oluyor ki\n",
    "        1001 desenini yakaladığımızda kelimeyi bulunduğumuz noktadan 2 ileriden\n",
    "        kesmeliyiz. ('1001', 2),\n",
    "        Kelime içinde yan yana gelen üç ünsüz harften ilk ikisi kendinden önceki\n",
    "        ünlüyle, üçüncüsü kendinden sonraki ünlüyle hece kurar. Yani 10001 desenini\n",
    "        gördüğümüzde kelimeyi bulunduğumuz yerden 3 ileri pozisyondan kesmemiz\n",
    "        gerek. ('10001', 3)\n",
    "        \"\"\"\n",
    "\n",
    "        seperators = (\n",
    "            ('101', 1),\n",
    "            ('1001', 2),\n",
    "            ('10001', 3)\n",
    "        )\n",
    "\n",
    "        index, cut_start_pos = 0, 0\n",
    "\n",
    "        # index değerini elimizdeki bitler üzerinde yürütmeye başlıyoruz.\n",
    "        while index < len(bits):\n",
    "\n",
    "            \"\"\"\n",
    "            Elimizdeki her ayırıcıyı (seperator), bits'in index'inci karakterinden\n",
    "            itibarent tek tek deneyerek yakalamaya çalışıyoruz.\n",
    "            \"\"\"\n",
    "\n",
    "            for seperator_pattern, seperator_cut_pos in seperators:\n",
    "                if bits[index:].startswith(seperator_pattern):\n",
    "\n",
    "                    \"\"\"\n",
    "                    Yakaladığımızda, en son cut_start posizyonundan, bulunduğumuz\n",
    "                    pozisyonun serpator_cut_pos kadar ilerisine kadar bölümü alıp\n",
    "                    syllables sepetine atıyoruz.\n",
    "                    \"\"\"\n",
    "\n",
    "                    syllables.append(word[cut_start_pos:index + seperator_cut_pos])\n",
    "\n",
    "                    \"\"\"\n",
    "                    Index'imiz seperator_cut_pos kadar ilerliyor, ve\n",
    "                    cut_start_pos'u index'le aynı yapıyoruz.\n",
    "                    \"\"\"\n",
    "\n",
    "                    index += seperator_cut_pos\n",
    "                    cut_start_pos = index\n",
    "                    break\n",
    "\n",
    "            \"\"\"\n",
    "            Index ilerliyor, cut_start_pos'da değişiklik yok.\n",
    "            \"\"\"\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        # Son kalan heceyi elle sepete atıyoruz.\n",
    "        syllables.append(word[cut_start_pos:])\n",
    "        return '-'.join(syllables)\n",
    "    def get_all_syllables(self):\n",
    "        return [self.__get_syllables_word(word) for word in self.__words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "    \n",
    "    def __init__(self, filePathOrData , syllableOrChar, isFile):\n",
    "        self.__type = syllableOrChar.lower()\n",
    "        self.__model = []\n",
    "        self.__data = \"\"\n",
    "        self.__path = filePathOrData\n",
    "        self.__readData()\n",
    "        \"\"\"if (isFile):\n",
    "            self.__path = filePathOrData\n",
    "            self.__readData()\n",
    "        else:\n",
    "            self.__path = \"\"\n",
    "            self.__data = filePathOrData \"\"\"   \n",
    "    \n",
    "    def __readData(self):\n",
    "        print(\"Reading Data...\")\n",
    "        self.__data = open(self.__path).read()\n",
    "    \n",
    "    def __process_text(self , text):\n",
    " \n",
    "        text = text.lower()\n",
    "        #text = text.replace(',', ' ')\n",
    "        text = text.replace('/', ' ')\n",
    "        text = text.replace('(', ' ')\n",
    "        text = text.replace(')', ' ')\n",
    "        text = text.replace('\\'' , ' ')\n",
    "        text = text.replace('\\\"' , '')\n",
    " \n",
    "        # Convert text string to a list of words\n",
    "        return text\n",
    "    \n",
    "    def __generate_ngrams_for_syllable(self, words_list, n):\n",
    "        ngrams_list = list()\n",
    "        for num in range(0, len(words_list)):\n",
    "            ngram = ' '.join(words_list[num:num + n])\n",
    "            ngrams_list.append(ngram)\n",
    "        print(\"Processing...\")\n",
    "        return ngrams_list\n",
    "\n",
    "    def __generate_ngrams_for_char(self , data, n):\n",
    "        ngrams_list = list()\n",
    "        for i in range(len(data)):\n",
    "            ngram = ''.join(data[i:i + n])\n",
    "            ngrams_list.append(ngram)\n",
    "        print(\"Processing...\")\n",
    "        return ngrams_list\n",
    "    \n",
    "    def __create_word_all_grams_with_probabilies(self ,data,n, function):\n",
    "        print(\"Model is creating...\")\n",
    "        #all_grams = list()\n",
    "        sorted_grams = list()\n",
    "        #for i in range(n + 1): # for find the all n grams 0,1,2,3,...n gram into model\n",
    "        n_grams = function(data, n)\n",
    "        n_counts = Counter(n_grams)\n",
    "        n_1_grams = function(data , n - 1)\n",
    "        n_1_counts = Counter(n_1_grams)\n",
    "        if(self.__type == \"character\"):\n",
    "            probs = {gram : (n_counts[gram] + 1)/(len(n_counts) + n_1_counts[''.join(gram[:-1])]) for gram in n_counts.keys()}\n",
    "        else:\n",
    "            probs = {gram : (n_counts[gram] + 1)/(len(n_counts) + n_1_counts[' '.join(gram.split()[:-1])]) for gram in n_counts.keys()}\n",
    "        \n",
    "        #sorted_grams.append(sorted(probs.items(), key=lambda kv: kv[1] , reverse=True))\n",
    "        self.__model.append(probs)\n",
    "        print(\"Model is created...\")\n",
    "        return self.__model #, sorted_grams\n",
    "    \n",
    "    def __syllable_perplexity(self , data):\n",
    "       \n",
    "        result = 1.0\n",
    "        #print(data)\n",
    "        tokens = data.split()\n",
    "        power = len(tokens)\n",
    "        next_gram = \"\"\n",
    "        for i in range(power - n + 1):\n",
    "            next_gram = ' '.join(tokens[i:i + self.__n])\n",
    "            if (self.__model[0].get(next_gram) == None):\n",
    "                result *= 1 / len(self.__model[0])\n",
    "            else:        \n",
    "                result *= self.__model[0].get(next_gram)\n",
    "        result = (1 / result) ** (1 / power)\n",
    "        return result\n",
    "        \n",
    "            \n",
    "    \n",
    "    def __char_perp(self , text):\n",
    "        result = 1.0\n",
    "        power = len(text)\n",
    "        for i in range(power - n + 1):\n",
    "            next_gram = ''.join(text[i:i + self.__n])\n",
    "            if(self.__model[0].get(next_gram) == None):\n",
    "                result *= 1 / len(self.__model[0])\n",
    "            else:\n",
    "                result *= self.__model[0].get(next_gram)\n",
    "        return (1 / result) ** (1 / power)\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"def __char_perplexity(self, data):\n",
    "        result = 1.0\n",
    "        power = len(tokens)\n",
    "        for i in range(power):\n",
    "            result *= self.__model[i + 1].get(''.join(tokens[:i + 1]))\n",
    "        result = (1 / result) ** (1 / power)\n",
    "        return result\"\"\"\n",
    "    \n",
    "    def save_model(self, name ):\n",
    "        print(\"{}-Gram model is saving...\".format(self.__n))\n",
    "        with open(name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load_model(name):\n",
    "        print(\"N-Gram model is loading...\")\n",
    "        with open(name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def create_NGram(self , n = 3):\n",
    "        print(\"{} {}-Gram model is creating...\".format(self.__type,n))\n",
    "        self.__n = n\n",
    "        \n",
    "        if(self.__type == \"character\"):\n",
    "            return self.__create_word_all_grams_with_probabilies(\n",
    "                             self.__process_text(self.__data),\n",
    "                             n,\n",
    "                             function = self.__generate_ngrams_for_char)\n",
    "        else:\n",
    "            data = self.__process_text(self.__data)\n",
    "            data = data.replace(\"-\" , \" \")\n",
    "            data = data.split()\n",
    "            return self.__create_word_all_grams_with_probabilies(\n",
    "                            data,\n",
    "                            n,\n",
    "                            function = self.__generate_ngrams_for_syllable)\n",
    "    \n",
    "    def perplexity(self , text):\n",
    "        text = text.lower()\n",
    "        if(self.__type == \"character\"):\n",
    "            return \"The perplexity of the character type n-gram for the given text : {}\".format(self.__char_perp(text))\n",
    "        else:\n",
    "            syllables = Syllable(text).get_all_syllables()\n",
    "            syllables_text = ' '.join(syllables)\n",
    "            return \"The perplexity of the character type n-gram for the given text : {}\".format(self.__syllable_perplexity(syllables_text.replace(\"-\" , \" \")))#for syllable\n",
    "        \n",
    "    \"\"\"def probability_of(self , text):\n",
    "        if(self.__type == \"character\"):\n",
    "            return self.__model[len(text)].get(text)\n",
    "        else:\n",
    "            return self.__model[len(text.split())].get(text)\"\"\"\n",
    "    \n",
    "    def getModel_info(self):\n",
    "        return self.__model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n",
      "syllable 4-Gram model is creating...\n",
      "Model is creating...\n",
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    input_file_char = \"data.txt\"\n",
    "    input_file_syllable = \"syllables_data.txt\"\n",
    "    model_type1 = \"character\"\n",
    "    model_type2 = \"syllable\"\n",
    "    n = 4\n",
    "    \n",
    "    \"\"\"model1 = NGram(input_file_char, model_type1)\n",
    "    \n",
    "    \n",
    "    model1.create_NGram(n)\n",
    "    model1.save_model(\"character_model1\")\"\"\"\n",
    "    \n",
    "    model2 = NGram(input_file_syllable , model_type2 , True)\n",
    "    model2.create_NGram(n)\n",
    "    model2.save_model(\"syllable_model_with_Laplace\")\n",
    "    \n",
    "    \n",
    "    #print(\"Perplexity of \\'{0}\\' is : {1}\".format(test_text , model.perplexity(test_text)))\n",
    "    \n",
    "    #print(\"Probability of \\'{0}\\' is : {1}\".format(test_text , model.probability_of(test_text)))\n",
    "    \n",
    "    #model_file = NGram.load_model(\"character_model1\")\n",
    "    \n",
    "    #model_file.new_perp(\"Eylül ayında yayın saatini arttıran\")\n",
    "    \n",
    "    #model_file.getModel_info()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = NGram.load_model(\"syllable_model1\")\n",
    "model_file.getModel_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file.perplexity(\"Yeşil renkli olarak görülebilirler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_syllable = \"turkish_file.txt\"\n",
    "model_type1 = \"character\"\n",
    "model_type2 = \"syllable\"\n",
    "n = 4\n",
    "\n",
    "\n",
    "#model2 = NGram(' '.join(Syllable(open(input_file_syllable).read()).get_all_syllables()) , model_type2 , False)\n",
    "model2 = NGram(input_file_syllable , model_type1 , True)\n",
    "\n",
    "\n",
    "model2.create_NGram(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2.perplexity(\"kalabalık\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
